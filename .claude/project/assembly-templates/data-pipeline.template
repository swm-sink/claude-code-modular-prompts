# Data Pipeline Assembly Template
# Template for data processing and transformation workflows

name: data-pipeline
description: Comprehensive data processing, transformation, and validation pipeline
category: data-processing
complexity: medium
estimated_time: "5-10 minutes"
target_users: "data engineers, analysts, ETL developers"

## Core Component Chain
components:
  primary_flow:
    - file-reader
    - input-validation
    - data-transformer
    - format-converter
    - output-formatter
    - response-validator
  
  supporting:
    - parameter-parser
    - content-sanitizer
    - error-handler
    - progress-indicator
    - workflow-coordinator

## Component Configurations

### file-reader
```yaml
mode: "batch_processing"
supported_formats: ["json", "csv", "xml", "yaml", "tsv"]
input_methods:
  - "single_file"
  - "directory_scan"
  - "recursive_search"
  - "pattern_matching"
  - "file_list"
large_file_handling: "streaming"
encoding_detection: "auto"
```

### input-validation
```yaml
validation_types:
  - "schema_validation"
  - "data_type_checking"
  - "range_validation"
  - "format_compliance"
  - "completeness_check"
schema_formats: ["json_schema", "avro", "protobuf", "custom"]
error_handling: "collect_and_continue"
validation_level: "strict"
```

### data-transformer
```yaml
transformation_types:
  - "field_mapping"
  - "data_cleansing"
  - "aggregation"
  - "filtering"
  - "enrichment"
  - "calculation"
operations:
  field_operations: ["rename", "split", "merge", "extract", "format"]
  data_operations: ["normalize", "deduplicate", "sort", "group", "pivot"]
  value_operations: ["trim", "uppercase", "lowercase", "replace", "calculate"]
parallel_processing: true
```

### format-converter
```yaml
input_formats: ["json", "csv", "xml", "yaml", "excel", "parquet"]
output_formats: ["json", "csv", "xml", "yaml", "excel", "parquet"]
conversion_options:
  preserve_schema: true
  optimize_storage: true
  maintain_relationships: true
custom_converters: true
```

### output-formatter
```yaml
output_styles:
  - "structured_json"
  - "tabular_csv"
  - "hierarchical_xml"
  - "human_readable"
  - "compressed"
formatting_options:
  pretty_print: true
  include_metadata: true
  add_timestamps: true
  compression: "auto"
```

### response-validator
```yaml
validation_checks:
  - "output_schema_compliance"
  - "data_integrity_check"
  - "completeness_verification"
  - "quality_metrics"
quality_thresholds:
  completeness_min: 95
  accuracy_min: 98
  consistency_min: 99
reporting: "detailed"
```

## Data Flow
```
Input Data Sources
    ↓
[file-reader] → Raw data ingestion
    ↓
[input-validation] → Data quality gates
    ↓
[data-transformer] → Business logic application
    ↓
[format-converter] → Format standardization
    ↓
[output-formatter] → Presentation layer
    ↓
[response-validator] → Quality assurance
    ↓
Final Output Products
```

## Assembly Instructions

### Quick Assembly
```bash
/assemble-command --from-template data-pipeline --quick
# Standard data processing with common configurations
```

### Format-Specific Assembly
```bash
/assemble-command --from-template data-pipeline --format csv-to-json
/assemble-command --from-template data-pipeline --format xml-to-csv
/assemble-command --from-template data-pipeline --format excel-to-parquet
```

### Interactive Customization
```bash
/assemble-command --from-template data-pipeline --customize
# Modify input/output formats, transformation rules, validation criteria
```

## Customization Points

### 1. Input Configuration
- **Single File**: Process one specific file
- **Batch Directory**: Process all files in a directory
- **Pattern Matching**: Process files matching specific patterns
- **Streaming**: Handle large files with streaming processing

### 2. Transformation Rules
- **Schema Mapping**: Map between different data structures
- **Data Cleansing**: Remove inconsistencies and errors
- **Business Logic**: Apply domain-specific transformations
- **Aggregation**: Summarize and group data

### 3. Output Options
- **Format Selection**: Choose optimal output format
- **Partitioning**: Split large datasets into manageable chunks
- **Compression**: Optimize storage and transfer
- **Metadata**: Include processing metadata and lineage

### 4. Quality Controls
- **Validation Rules**: Define data quality expectations
- **Error Handling**: Configure failure recovery strategies
- **Monitoring**: Set up processing metrics and alerts
- **Auditing**: Track data lineage and transformations

## Use Case Examples

### CSV Data Cleansing
```yaml
input: "raw_customer_data.csv"
transformations:
  - "normalize_phone_numbers"
  - "validate_email_addresses"
  - "standardize_addresses"
  - "remove_duplicates"
output: "clean_customer_data.json"
validation: "strict_schema_compliance"
```

### JSON to Database Import
```yaml
input: "api_responses/*.json"
transformations:
  - "flatten_nested_objects"
  - "extract_key_fields"
  - "add_import_timestamps"
  - "validate_foreign_keys"
output: "database_import.sql"
validation: "referential_integrity"
```

### Excel Report Processing
```yaml
input: "monthly_reports/*.xlsx"
transformations:
  - "extract_summary_sheets"
  - "calculate_derived_metrics"
  - "standardize_date_formats"
  - "aggregate_by_region"
output: "consolidated_report.csv"
validation: "completeness_check"
```

## Expected Outcomes

### Deliverables
1. **Processed Data Files** (in requested format)
2. **Processing Report** (success/failure metrics)
3. **Data Quality Report** (validation results)
4. **Transformation Log** (detailed processing steps)

### Performance Metrics
- **Processing Speed**: 1000-10000 records/second (depending on complexity)
- **Memory Efficiency**: <500MB for most datasets
- **Error Rate**: <1% with proper validation
- **Data Quality Score**: >95% completeness, >98% accuracy

### Quality Assurance
- **Input Validation**: 100% of input data validated
- **Transformation Verification**: All rules applied correctly
- **Output Verification**: Schema and format compliance
- **End-to-End Testing**: Complete pipeline validation

## Integration Examples

### Batch Processing
```bash
# Daily data processing job
/assemble-command --from-template data-pipeline \
  --input "daily_files/*.csv" \
  --output "processed/$(date +%Y-%m-%d)" \
  --schedule daily
```

### Real-time Processing
```bash
# Streaming data pipeline
/assemble-command --from-template data-pipeline \
  --mode streaming \
  --input-stream "kafka://data-topic" \
  --output-stream "processed-topic"
```

### API Integration
```bash
# Process API responses
/assemble-command --from-template data-pipeline \
  --input-api "https://api.example.com/data" \
  --transform "api_response_normalization" \
  --output "database://processed_data"
```

## Error Handling Strategies

### Data Quality Issues
- **Missing Values**: Fill with defaults, interpolate, or flag for review
- **Format Errors**: Auto-correct when possible, quarantine problematic records
- **Schema Violations**: Transform to comply or create exception reports

### Processing Failures
- **Partial Success**: Process what's possible, report failures
- **Retry Logic**: Automatic retry with exponential backoff
- **Fallback Options**: Alternative processing paths for edge cases

## Success Criteria
- ✅ **High throughput** (>1000 records/second for typical datasets)
- ✅ **Data integrity** maintained throughout pipeline
- ✅ **Comprehensive validation** at each stage
- ✅ **Flexible configuration** for various data sources and formats
- ✅ **Robust error handling** with detailed reporting
- ✅ **Scalable architecture** for growing data volumes